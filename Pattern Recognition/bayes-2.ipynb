{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.priors = None\n",
    "        self.M_s=0\n",
    "        self.Perror=0\n",
    "        self.M = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = X.shape[1]\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.variances = np.zeros((n_classes, n_features))   \n",
    "        self.priors = np.zeros(n_classes)\n",
    "    \n",
    "        # 計算先驗機率\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.priors[i] = X_c.shape[0] / X.shape[0]\n",
    "            if X_c.shape[0] == 0:\n",
    "                self.means[i] = np.zeros(n_features)\n",
    "                self.variances[i] = np.zeros(n_features)\n",
    "            else:\n",
    "                self.means[i] = np.mean(X_c, axis=0)\n",
    "                self.variances[i] = np.var(X_c, axis=0) + 1e-9\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1) + 1\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        likelihood = np.zeros((X.shape[0], len(self.classes)))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            class_likelihood = stats.norm.pdf(X, loc=self.means[i], scale=np.sqrt(self.variances[i]))\n",
    "            class_likelihood = np.prod(class_likelihood, axis=1)\n",
    "            likelihood[:, i] = class_likelihood * self.priors[i]\n",
    "        evidence = np.sum(likelihood * self.priors, axis=1) + 1e-10\n",
    "        return likelihood / evidence.reshape(-1, 1)\n",
    "\n",
    "    def fit2(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = X.shape[1]\n",
    "        self.means = np.zeros((n_classes, n_features))\n",
    "        self.variances = np.zeros((n_classes, n_features, n_features))   \n",
    "        self.priors = np.zeros(n_classes)\n",
    "\n",
    "        # 計算先驗機率\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.priors[i] = X_c.shape[0] / X.shape[0]\n",
    "            if X_c.shape[0] == 0:\n",
    "                self.means[i] = np.zeros(n_features)\n",
    "                self.variances[i] = np.zeros((n_features, n_features))\n",
    "            else:\n",
    "                self.means[i] = np.mean(X_c, axis=0)\n",
    "                self.variances[i] = np.cov(X_c, rowvar=False) + 1e-6*np.eye(n_features)\n",
    "    \n",
    "\n",
    "    def Bayes_error(self):\n",
    "        # initialize M matrix\n",
    "        n_classes = len(self.classes)\n",
    "            \n",
    "        if len(self.variances.shape) == 1:\n",
    "            self.variances = self.variances.reshape(-1, 1)\n",
    "\n",
    "        M = np.zeros((n_classes, n_classes))\n",
    "\n",
    "        # calculate M matrix entries using loops\n",
    "        for i in range(n_classes):\n",
    "            for j in range(i+1, n_classes):\n",
    "                delta = self.means[i]-self.means[j]\n",
    "                sigma = 0.5*(self.variances[i]+self.variances[j])\n",
    "                sigma_inv = np.linalg.pinv(sigma)\n",
    "                m_ij = 0.125 * np.dot(delta.T, np.dot(sigma_inv, delta)) + 0.5 * np.log(np.linalg.det(sigma) / np.sqrt(np.linalg.det(self.variances[i]) * np.linalg.det(self.variances[j])))\n",
    "                M[i,j] = m_ij\n",
    "                M[j,i] = m_ij\n",
    "\n",
    "        s_values = np.linspace(0.1, 0.9, 20)\n",
    "        \n",
    "        # 找出最佳的 s 值\n",
    "        opt_s = 0\n",
    "        min_error = float('inf')\n",
    "        \n",
    "        for s in s_values:    \n",
    "            Perrors = []\n",
    "            for i in range(3):\n",
    "                for j in range(i+1, 3):\n",
    "                    alpha_ij = math.sqrt(np.linalg.det(s*self.variances[i]+(1-s)*self.variances[j])/np.sqrt(np.linalg.det(self.variances[i])*np.linalg.det(self.variances[j])))\n",
    "                    Perror = 0.25 * (1 - alpha_ij) * ((self.priors[i]*math.sqrt(s) - self.priors[j]*math.sqrt(1-s))**2 + (self.priors[i]*math.sqrt(1-s) - self.priors[j]*math.sqrt(s))**2)\n",
    "                    Perrors.append(Perror)\n",
    "            \n",
    "            # 输出结果\n",
    "            print(f\"For s={s}:\")\n",
    "            for i in range(3):\n",
    "                for j in range(i+1, 3):\n",
    "                    print(f\"Perror({i+1},{j+1}) = {Perrors[i*2+j-3]}\")\n",
    "            \n",
    "            # 找到更好的 s 值時更新最佳 s 值與對應的誤差率\n",
    "            min_idx = np.argmin(Perrors)\n",
    "            if Perrors[min_idx] < min_error:\n",
    "                min_error = Perrors[min_idx]\n",
    "                opt_s = s_values[min_idx // 3]\n",
    "                \n",
    "        print(\"Optimal s: {}\".format(opt_s))\n",
    "        print(\"Bayes error rate: {}\".format(min_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayes:\n",
    "    def __init__(self, training_data, labels):\n",
    "        self.training_data = training_data\n",
    "        self.labels = labels\n",
    "        self.classes = np.unique(labels)\n",
    "        self.class_num = len(self.classes)\n",
    "        self.feature_num = training_data.shape[1]\n",
    "        self.means = np.zeros([self.class_num, self.feature_num])\n",
    "        self.variances = np.zeros([self.class_num, self.feature_num])\n",
    "        self.priors = [0 for _ in range(self.class_num)]\n",
    "\n",
    "    def fit(self):\n",
    "        #calculate priors, means, variances\n",
    "        for i, c in enumerate(self.classes):\n",
    "            data_class = self.training_data[self.labels == c]\n",
    "            self.priors[i] = len(data_class) / len(self.training_data)\n",
    "            self.means[i] = np.mean(data_class, axis=0)\n",
    "            self.variances[i] = np.var(data_class, axis=0) + 1e-10\n",
    "\n",
    "    def predict(self, testing_data):\n",
    "        likelihood = np.zeros([len(testing_data), self.class_num])\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            class_likelihood = stats.norm.pdf(testing_data, loc=self.means[i], scale=np.sqrt(self.variances[i]))\n",
    "            class_likelihood = np.prod(class_likelihood, axis=1)\n",
    "            likelihood[:, i] = class_likelihood * self.priors[i]\n",
    "        return self.classes[np.argmax(likelihood, axis=1)]\n",
    "\n",
    "    def score(self, testing_data, testing_labels):\n",
    "        pre_labels = self.predict(testing_data)\n",
    "        return np.sum([pre_labels[i] == testing_labels[i] for i in range(len(testing_data))]) / len(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9438202247191011"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定特徵名稱\n",
    "feature_names = ['label',\n",
    "                 'Alcohol', \n",
    "                 'Malic acid',\n",
    "                 'Ash',\n",
    "                 'Alcalinity of ash' ,\n",
    "                 'Magnesium',\n",
    "                 'Total phenols',\n",
    "                 'Flavanoids',\n",
    "                 'Nonflavanoid phenols',\n",
    "                 'Proanthocyanins',\n",
    "                 'Color intensity',\n",
    "                 'Hue',\n",
    "                 'OD280/OD315 of diluted wines',\n",
    "                 'Proline' ]\n",
    "data=pd.read_csv(\"wine.txt\", names=feature_names)\n",
    "data=data.sample(frac=1)\n",
    "\n",
    "half = int(len(data) * 0.5)\n",
    "train_data=data.iloc[:half]\n",
    "test_data=data.iloc[half:]\n",
    "\n",
    "train_labels = train_data.iloc[:,0].values\n",
    "test_labels = test_data.iloc[:,0].values\n",
    "\n",
    "train_data = train_data.drop('label', axis=1)\n",
    "test_data = test_data.drop('label', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data.iloc[:, :])\n",
    "test_data  = scaler.transform(test_data.iloc[:, :])\n",
    "\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(train_data, train_labels)\n",
    "pred = nb.predict(test_data)\n",
    "score = np.sum([pred[i] == test_labels[i] for i in range(len(test_data))]) / len(test_data)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(train_data, train_labels)\n",
    "pred = nb.predict(test_data)\n",
    "score_nb = np.sum([pred[i] == test_labels[i] for i in range(len(test_data))]) / len(test_data)\n",
    "\n",
    "test = Bayes(train_data, train_labels)\n",
    "test.fit()\n",
    "pred = test.predict(test_data)\n",
    "score_test = test.score(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39722144 0.2712674  0.51017867 0.33763694 0.32831658 0.22213115\n",
      "  0.11055647 0.15836656 0.51917201 0.23688232 0.24208257 0.29802727\n",
      "  0.47191573]\n",
      " [0.50289638 1.16433263 1.39850484 1.03224218 1.60691685 0.86470027\n",
      "  0.60248285 0.93180666 1.01955307 0.17351018 0.75222949 0.39279083\n",
      "  0.28961617]\n",
      " [0.38411331 0.81690072 0.41517506 0.36962345 0.36062219 0.53877142\n",
      "  0.12341206 1.15014068 0.73278312 1.08258178 0.2586773  0.13044894\n",
      "  0.13301325]]\n",
      "[[0.39722144 0.2712674  0.51017867 0.33763694 0.32831658 0.22213115\n",
      "  0.11055647 0.15836656 0.51917201 0.23688232 0.24208257 0.29802727\n",
      "  0.47191572]\n",
      " [0.50289638 1.16433263 1.39850484 1.03224218 1.60691685 0.86470027\n",
      "  0.60248285 0.93180666 1.01955307 0.17351018 0.75222949 0.39279083\n",
      "  0.28961616]\n",
      " [0.38411331 0.81690072 0.41517506 0.36962345 0.36062219 0.53877141\n",
      "  0.12341206 1.15014068 0.73278312 1.08258178 0.2586773  0.13044894\n",
      "  0.13301325]]\n",
      "0.9438202247191011 0.9438202247191011\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e72190538c2ba43f01a4bb1f8db058440a9aceb794ef196497a6a1a579de28d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
